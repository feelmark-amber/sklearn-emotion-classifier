# モチベーション維持重視！【ライブラリ初心者向け】実装計画案 (「有無」方式 再構成版)

**現在の前提:**

* **スキル:** Python基本文法OK、主要ライブラリ未経験。
* **アノテーション:** 各テキストに対し、27感情カテゴリーから該当するものを**全て選択**（有無、0/1）。
* **目標:** 感情分類AIを作り、小説推薦へ繋げる。モチベーション維持最優先。

---

## フェーズ 0: Python基礎の活用と準備運動 (ライブラリ未使用)

* **目的:** Python基礎だけでデータ構造や入出力形式を体感。
* **手順:**
    * Python環境確認。
    * 【重要】感情カテゴリーリスト(27個)と、「有無」を判断するための簡単な定義をファイルにまとめる。
    * 手作業でのデータ扱い体験:
        * 5〜10個のサンプルテキストと、それに対応する感情ラベル（**存在するラベル名のリスト**形式）をPythonスクリプト内に `list[dict]` で作成。例: `{'id': 1, 'text': '最高！', 'labels': ['Joy', 'Amusement']}`
        * Python基本操作: ループ、条件分岐、リスト/辞書操作でデータ表示、特定ラベル（例: 'Joy'）の有無チェック、存在するラベルリスト形式での出力シミュレーション。標準ライブラリ `json` での出力も試す。
* **成果:** プロジェクトデータの基本形、処理の流れ、「有無」ラベルリスト形式の出力イメージを掴む。
## フェーズ 1: データ整理の武器を学ぶ - Pandas入門

* **目的:** データ処理ライブラリ「Pandas」の基本習得。
* **手順:**
    * Pandas学習: `pip install pandas`。`DataFrame`作成、CSV読み書き、データ選択、`value_counts()`などを学ぶ。
    * CSVデータ作成: 20〜30個のサンプルテキストと、**27感情カテゴリーそれぞれに対応する0/1の列**を持つCSV (`sample_data_binary.csv`) を作成。
    * Pandasでデータ操作: CSVを読み込み、特定の感情が「1」になっているテキストを抽出するなど、基本的なデータ操作を練習。
* **成果:** Pandas基本操作スキル。0/1形式のラベルデータCSVの扱い方習得。

## フェーズ 2: "なんちゃってAI" - ルールベース分類器作成

* **目的:** 簡単なロジックでの分類体験、入力→処理→出力の流れ把握。
* **手順:**
    * 単純なルール設定: 各感情に結びつくキーワードを設定。
    * ルールベース分類器作成 (`rule_classifier.py`): テキスト内のキーワードをチェックし、**該当する感情ラベル名のリスト**を返す関数を作成。
    * テスト: フェーズ1のCSVデータをPandasで読み、この分類器で処理してみる。
* **成果:** 最も基本的な分類プログラム。「有無」分類のロジック理解。

## フェーズ 3: AIの世界へ - Hugging Face入門と最初のモデル体験

* **目的:** Transformersライブラリ体験、最小限のファインチューニング挑戦（「有無」方式で）。
* **手順:**
    * Hugging Face `pipeline` を試す: `pip install transformers[torch]` (または `tensorflow`)。`pipeline`関数で簡単なテキスト分類を体験。
    * 最小ファインチューニング挑戦:
        * データ: フェーズ1の `sample_data_binary.csv` (0/1形式) を使用。
        * 学習リソース: Hugging Faceの**テキスト分類チュートリアル**（特にマルチラベル分類の例があればベスト）を**一行ずつ理解しながら**進める。`Trainer` APIを使う。
        * スクリプト作成 (`train_minimal.py`): チュートリアルを元に、**マルチラベル分類（0/1）**用に設定（損失関数: `BCEWithLogitsLoss` が一般的）。自分のデータで動くように**少しずつ**書き換える。
        * 実行: 1〜2エポック学習させ、新しいテキストで推論（各ラベルの確率が出力されるはず）を試す。**結果よりプロセス完遂を目指す。**
* **成果:** `pipeline`利用体験。`Trainer`を使った**「有無」マルチラベル**の小規模ファインチューニング体験。動いたコード (`train_minimal.py`)。

## フェーズ 4: 本格的なモデル構築へ - データ作成とV1モデル学習

* **目的:** **「有無」形式でアノテーションされたデータ**で、実用を目指した最初の感情分類モデルを構築する。
* **手順:**
    * 詳細アノテーションガイドライン作成: 「有無」判断基準を明確にしたガイドラインを作成。
    * データ収集と【「有無」】アノテーション（第一弾）: 数百〜1000件のテキストデータを収集し、ガイドラインに基づき**「有無」（0/1）でアノテーション。Pandasで管理するCSVを作成。（スコア付けより格段に楽なはず！）**
    * 評価指標の学習: マルチラベル分類の評価指標（F1スコア(micro/macro/samples), Hamming Lossなど）を学ぶ。`scikit-learn`（`pip install scikit-learn`）での計算方法も学ぶ。
    * 学習スクリプトの改良 (`train_v1.py`): フェーズ3のスクリプトを元に、`Trainer` の `compute_metrics` で評価指標を計算・表示、モデル保存機能を追加。
    * V1モデル学習: 作成した「有無」データセットでモデル（例: `cl-tohoku/bert-base-japanese-whole-word-masking`）をファインチューニング。GPU環境推奨（Colab Proなど）。
    * 結果の確認: 評価指標を見て、モデルの性能を確認。
* **成果:** **「有無」マルチラベル**感情分類モデル (V1)。評価方法習得。本格的な学習スクリプト (`train_v1.py`)。

## フェーズ 5: モデルを届ける - Web API入門 (FastAPI/Flask)

* **目的:** 学習済みモデルをAPI化する。
* **手順:**
    * Webフレームワーク学習: `pip install fastapi uvicorn[standard]` または `pip install Flask`。基本を学ぶ。
    * 推論関数の準備: V1モデルを読み込み、テキストを受け取って**各感情ラベルの確率（0.0〜1.0）**を返す関数を作成。
    * APIサーバー作成 (`main.py`):
        * 推論関数を呼び出すエンドポイントを作成。
        * モデルから返された確率に基づき、**閾値（例: 0.5）を超えた感情ラベルのリスト**をJSON形式で返すようにする（あるいは、確率自体を返すオプションも）。
    * 動作テスト: `curl`やPython `requests`（`pip install requests`）でAPIをテスト。
* **成果:** 動作するWeb API。モデルの出力を外部から利用可能に。

## フェーズ 6以降: 応用と改善

* **作品プロファイリング:** レビューをAPIで分析し、各作品で**どの感情ラベルがよく出現するか（例: 閾値超えの頻度）**を集計してプロファイル作成。
* **推薦MVP:** ユーザー検索クエリをAPIで分析し、得られた感情ラベルリストと、作品プロファイルの感情リストが**類似する作品**（例: Jaccard係数などで類似度計算）を推薦するロジック作成。
* **反復的改善:** データ追加（「有無」アノテーション）、モデル再学習、評価を継続。

---
## おすすめのPythonプロジェクトフォルダ構成

emotion_classifier_project/
│
├── .gitignore             # Git管理外のファイル指定
├── README.md              # プロジェクト説明・使い方
├── requirements.txt       # Pythonライブラリ依存関係リスト
│
├── data/                  # データ関連フォルダ
│   ├── guidelines/        # アノテーションガイドライン等
│   │   └── annotation_guidelines_binary.md # 「有無」方式のガイドライン
│   ├── raw/               # 生データ
│   ├── processed/         # 前処理済みデータ
│   ├── annotated/         # アノテーション済み学習データ (0/1形式のCSVなど)
│   └── book_profiles/     # (フェーズ6以降) 作品感情プロファイル
│
├── models/                # 学習済みモデル保存フォルダ (※.gitignore推奨)
│   └── emotion_classifier_v1_binary/
│       └── ... (モデルファイル)
│
├── notebooks/             # 実験・探索用Jupyter Notebookフォルダ
│   ├── 01_data_exploration.ipynb
│   └── 02_rule_based_test.ipynb
│
├── scripts/               # 個別処理用スクリプトフォルダ
│   ├── 01_prepare_data.py
│   └── 02_profile_books.py
│
├── src/                   # メインのPythonソースコードフォルダ
│   ├── init.py        # このフォルダをPythonパッケージとして認識させる
│   ├── training/          # モデル学習関連コード
│   │   ├── init.py
│   │   ├── train.py         # train_minimal.py や train_v1.py の最終版
│   │   └── metrics.py       # 評価指標計算の関数など
│   ├── inference/         # 推論（予測）関連のコード
│   │   ├── init.py
│   │   └── predictor.py     # モデルを読み込んで予測を行うクラスや関数
│   ├── api/               # APIサーバー関連のコード
│   │   ├── init.py
│   │   └── main.py          # FastAPI/Flask のメインファイル
│   ├── recommendation/    # (フェーズ6以降) 推薦ロジック関連コード
│   │   ├── init.py
│   │   └── recommend.py
│   └── utils/             # 複数の場所で使われる共通のヘルパー関数など
│       ├── init.py
│       └── helpers.py
│
└── venv/                  # Python仮想環境フォルダ (※.gitignore推奨)
└── ... (ライブラリなど)

### 各フォルダ・ファイルの説明:

* **`.gitignore`**: `models/`, `data/raw/`, `venv/`, `__pycache__/`などを指定し、Gitリポジトリをきれいに保ちます。
* **`README.md`**: プロジェクトの「顔」。目的、セットアップ方法、使い方を記載します。
* **`requirements.txt`**: プロジェクトの依存ライブラリリスト (`pip freeze > requirements.txt` で作成)。環境再現に必須。
* **`data/`**: コードとデータを分離。生データ(`raw`)、加工データ(`processed`)、アノテーション済み学習データ(`annotated` - **0/1形式**)、成果物(`book_profiles`)、ガイドライン(`guidelines`)を整理。
* **`models/`**: 学習済みモデルファイルを保存 (Gitには含めない推奨)。
* **`notebooks/`**: Jupyter Notebook でのデータ探索や実験用。初心者には特におすすめ。
* **`scripts/`**: データ準備やバッチ処理など、単発実行するスクリプト用。
* **`src/`**: 再利用可能なメインのPythonコード。機能ごとにサブフォルダに分けることで見通しが良くなります。
    * `__init__.py` : 各フォルダをPythonパッケージとして認識させます（空でOK）。
    * `training/`: **「有無」データ**で学習させるコード。
    * `inference/`: 学習済みモデルで**確率を出力**するコード。
    * `api/`: 確率を元に**「有無」ラベルリストなどを返す**APIコード。
    * `recommendation/`: **「有無」ラベルの出現頻度など**を元にした推薦コード。
    * `utils/`: 共通の補助関数。
* **`venv/`**: プロジェクト専用のPython仮想環境。

---

この計画とフォルダ構成で、ぜひプロジェクトを進めてみてください。